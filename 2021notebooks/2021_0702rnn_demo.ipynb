{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021_0702rnn_demo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPKsQFeDtDOT8vcYcXQXwYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/project-ccap/ccap/blob/master/2021notebooks/2021_0702rnn_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVvUyfiPvbft"
      },
      "source": [
        "# RNN による言語モデルのデモ\n",
        "- author: 浅川伸一\n",
        "- date: 2021_0702\n",
        "- filename: 2021_0702rnn_demo.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6_b7Zp3sPy5"
      },
      "source": [
        "#図中に日本語の説明を入れるためのライブラリをインストール\n",
        "!pip install japanize_matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJAE8ldarxjh"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "\n",
        "#ソフトマックス関数の定義\n",
        "def softmax(x, beta=1):\n",
        "    xt = np.exp(beta * x - np.mean(beta * x))\n",
        "    return xt / np.sum(xt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvD-09kWrkzK"
      },
      "source": [
        "class srn_model():\n",
        "    \n",
        "    def __init__(self,\n",
        "                 n_hid = 200,         #中間層のサイズ\n",
        "                 lr = 1e-1,           #学習率\n",
        "                 max_epochs=10 ** 3,  #最大エポック数\n",
        "                 interval=None,       #途中結果の表示タイミング\n",
        "                 plot=True, \n",
        "                 cont=False, \n",
        "                 text=None,\n",
        "                 generate_text=True,\n",
        "                 chr2idx=None):\n",
        "    \n",
        "        # ハイパーパラメータ\n",
        "        self.n_hid = n_hid             # 中間層のニューロン数\n",
        "        self.lr = lr\n",
        "        self.plot = plot\n",
        "        self.chr2idx = chr2idx\n",
        "        self.max_epochs = max_epochs\n",
        "        self.interval = interval if interval != None else self.max_epochs >> 3\n",
        "        self.generate_text = generate_text\n",
        "        self.smooth_loss = None\n",
        "        self.looses = []\n",
        "        \n",
        "        if text == None: #何も指定されていない場合，データを用意する\n",
        "            text = \"\"\"\n",
        "日本国民は、正義と秩序を基調とする国際平和を誠実に希求し、\\n\n",
        "国権の発動たる戦争と、武力による威嚇又は武力の行使は、\\n\n",
        "国際紛争を解決する手段としては、永久にこれを放棄する。\\n\n",
        "前項の目的を達するため、陸海空軍その他の戦力は、\\n\n",
        "これを保持しない。\\n\n",
        "国の交戦権は、これを認めない。\"\"\"\n",
        "            text = text.split()\n",
        "\n",
        "        self.text = text\n",
        "        self.seq_len = len(self.text)\n",
        "\n",
        "        if self.chr2idx == None:\n",
        "            self.make_chr2idx(text)\n",
        "        else:\n",
        "            self.idx2char = {i:c for i,c in self.chr2idx}\n",
        "\n",
        "        self.X = np.zeros((len(text), self.max_len), dtype=np.uint)\n",
        "        ones = np.ones((len(text),1), dtype=np.uint)\n",
        "        self.X = np.concatenate((ones, self.X), axis=1)\n",
        "        for i, line in enumerate(text):\n",
        "            for j, chr in enumerate(line):\n",
        "                self.X[i,j] = self.chr2idx[chr]\n",
        "\n",
        "        # 結合係数行列とバイアス項の初期化\n",
        "        self.Wxh = np.random.randn(self.n_hid, self.n_vocab) / np.sqrt(self.n_vocab + self.n_hid)  # input to hidden\n",
        "        self.Whh = np.random.randn(self.n_hid, self.n_hid) / np.sqrt(self.n_hid + self.n_hid)      # hidden to hidden\n",
        "        self.Why = np.random.randn(self.n_vocab, self.n_hid) / np.sqrt(self.n_hid + self.n_vocab)  # hidden to output\n",
        "        self.bh = np.zeros((self.n_hid, 1))                                          # hidden bias\n",
        "        self.by = np.zeros((self.n_vocab, 1))                                        # output bias\n",
        "            \n",
        "\n",
        "    def make_chr2idx(self, text):\n",
        "        #chars = ['<eow>', '<sow>']  # <eow>: end of word, <sow>: start of word\n",
        "        chars = ['<eow>']             # <eow>: end of word, <sow>: start of word\n",
        "        for line in text:\n",
        "            [chars.append(chr) for chr in line]\n",
        "        \n",
        "        self.all_chars = set(chars)\n",
        "        self.n_vocab = len(self.all_chars)\n",
        "        \n",
        "        self.chr2idx, self.idx2chr = {}, {}\n",
        "        self.chr2idx['<eow>'] = 0   #, self.chr2idx['<sow>'] = 0, 1\n",
        "        self.idx2chr[0] = '<eow>'   #, self.idx2chr[1] = '<eow>', '<sow>'\n",
        "        idx = 1                     #+1 <eow> # +2 にしていあるのは <eow> <sow> のため\n",
        "        max_len = 0\n",
        "        for line in text:\n",
        "            if max_len < len(line):\n",
        "                max_len = len(line)\n",
        "            for chr in line:\n",
        "                if not chr in self.chr2idx:\n",
        "                    self.chr2idx[chr] = idx \n",
        "                    self.idx2chr[idx] = chr\n",
        "                    idx += 1\n",
        "                \n",
        "        self.max_len =  max_len + 1  # +1 にしていあるのは <eow> のため\n",
        "\n",
        "    \n",
        "    def init_memory(self):\n",
        "        # m で始まる変数は，Adagrad で用いるメモリ変数。\n",
        "        self.mWxh = np.zeros_like(self.Wxh)  # mWxh: 入力から中間層への結合係数行列\n",
        "        self.mWhh = np.zeros_like(self.Whh)  # mWhh: 中間層へのリカレント結合係数行列\n",
        "        self.mWhy = np.zeros_like(self.Why)  # mWhy: 中間層から出力層への結合係数行列\n",
        "        self.mbh = np.zeros_like(self.bh)    # mbh: 中間層のバイアス項\n",
        "        self.mby = np.zeros_like(self.by)    # mby: 出力層のバイアス項\n",
        "\n",
        "    \n",
        "    def forward(self, inputs, targets, h_prev):\n",
        "        \"\"\"inputs,targets are both list of integers.\n",
        "        hprev is Hx1 array of initial hidden state\n",
        "        returns the loss, gradients on model parameters, and last hidden state\n",
        "        \"\"\"\n",
        "        x, h, y, prob = {}, {}, {}, {}\n",
        "        h[-1] = np.copy(h_prev)\n",
        "        loss = 0\n",
        "        for t in range(len(inputs)):\n",
        "            x[t] = np.zeros((self.n_vocab,1)) # encode in 1-of-k representation\n",
        "            x[t][inputs[t]] = 1\n",
        "            \n",
        "            h[t] = np.tanh(np.dot(self.Wxh, x[t]) \n",
        "                           + np.dot(self.Whh, h[t-1]) \n",
        "                           + self.bh)\n",
        "            y[t] = np.dot(self.Why, h[t]) + self.by  # unnormalized log probabilities for next chars\n",
        "            prob[t] = softmax(y[t]) \n",
        "            loss += -np.log(prob[t][targets[t],0])   # softmax (cross-entropy loss)  \n",
        "\n",
        "        return loss, x, h, y, prob\n",
        "\n",
        "\n",
        "    def backward(self, inputs, targets, hprev, x_, h_, y_, prob_):\n",
        "        \"\"\"backward pass: compute gradients going backwards\"\"\"\n",
        "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
        "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
        "        dhnext = np.zeros_like(h_[0])\n",
        "    \n",
        "        for t in reversed(range(len(inputs))):\n",
        "            dy = np.copy(prob_[t])\n",
        "            dy[targets[t]] -= 1      \n",
        "            dWhy += np.dot(dy, h_[t].T)\n",
        "            dby += dy\n",
        "            \n",
        "            dh = np.dot(self.Why.T, dy) + dhnext  # backprop into h\n",
        "            delta = (1 - h_[t] * h_[t]) * dh      # backprop through tanh nonlinearity\n",
        "            \n",
        "            dbh  += delta\n",
        "            dWxh += np.dot(delta, x_[t].T)\n",
        "            dWhh += np.dot(delta, h_[t-1].T)\n",
        "            dhnext = np.dot(self.Whh.T, delta)\n",
        "                    \n",
        "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "            np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
        "        \n",
        "        return dWxh, dWhh, dWhy, dbh, dby, h_[len(inputs)-1]\n",
        "    \n",
        "    \n",
        "    def sample(self, hprev, seed, n):\n",
        "        \"\"\" \n",
        "        sample a sequence of integers from the model \n",
        "        h is memory state, seed_ix is seed letter for first time step\n",
        "        \"\"\"\n",
        "        x = np.zeros((self.n_vocab, 1))\n",
        "        hid = hprev\n",
        "        x[seed] = 1\n",
        "        idxes = []\n",
        "        for t in range(n):\n",
        "            hid = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, hid) + self.bh)\n",
        "            out = np.dot(self.Why, hid) + self.by\n",
        "            prob = softmax(out)\n",
        "            idx = np.random.choice(range(self.n_vocab), p=prob.ravel())\n",
        "            x = np.zeros((self.n_vocab, 1))\n",
        "            x[idx] = 1\n",
        "            idxes.append(idx)\n",
        "        return idxes\n",
        "\n",
        "    \n",
        "    def generate(self, hprev, seed, n):\n",
        "        \"\"\"generate a sequence of integers from the model \n",
        "        h is memory state, seed_ix is seed letter for first time step\n",
        "        \"\"\"\n",
        "        x = np.zeros((self.n_vocab, 1))\n",
        "        hid = hprev\n",
        "        x[seed] = 1\n",
        "        idxes = []\n",
        "        for t in range(n):\n",
        "            hid = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, hid) + self.bh)\n",
        "            out = np.dot(self.Why, hid) + self.by\n",
        "            prob = softmax(out)\n",
        "            idx = np.argmax(prob)\n",
        "            x = np.zeros((self.n_vocab, 1))\n",
        "            x[idx] = 1\n",
        "            idxes.append(idx)\n",
        "        return idxes\n",
        "\n",
        "\n",
        "    def draw_loss_plot(self, losses):\n",
        "        plt.xlabel('訓練回数'); plt.ylabel('損失'); plt.title('損失値の変化')\n",
        "        plt.plot(losses); plt.show()\n",
        "\n",
        "\n",
        "    def all_text_generation(self):\n",
        "        for i_ in range(len(self.X)):\n",
        "            line = self.X[i_]\n",
        "            hprev = np.zeros((self.n_hid,1)) # reset RNN memory\n",
        "            h_prev = np.zeros((self.n_hid, 1))\n",
        "                \n",
        "            stop_no = line.tolist().index(self.chr2idx['<eow>'])\n",
        "            inputs = line[:stop_no]\n",
        "            sample_idx = self.generate(h_prev, inputs[0], self.X.shape[1])\n",
        "            out_txt = self.idx2chr[inputs[0]]\n",
        "            for idx in sample_idx:\n",
        "                ch = self.idx2chr[idx]\n",
        "                if ch == '<eow>':\n",
        "                    break\n",
        "                else:\n",
        "                    out_txt += ch\n",
        "            print(i_, out_txt)\n",
        "\n",
        "\n",
        "    def train(self, plot=None, generate_text=None, max_epochs=None, interval=None):\n",
        "        self.plot = plot if plot != None else self.plot\n",
        "        self.generate_text = generate_text if generate_text != None else self.generate_text\n",
        "        if max_epochs != None:\n",
        "            self.max_epochs = max_epochs\n",
        "            \n",
        "        if interval != None:\n",
        "            self.interval = interval\n",
        "\n",
        "        if self.smooth_loss == None:\n",
        "            self.smooth_loss = -np.log(1.0/self.n_vocab) * len(self.X) * self.max_len\n",
        "            self.losses = []\n",
        "            self.init_memory()\n",
        "        \n",
        "        for epoch in range(self.max_epochs):\n",
        "            idxes = np.random.permutation(len(self.X))  # シャッフル\n",
        "            t_loss = 0\n",
        "            for num in range(len(self.X)):\n",
        "                line = self.X[idxes[num]]\n",
        "                \n",
        "                seq, seq_no = [], 0\n",
        "                stop_no = line.tolist().index(self.chr2idx['<eow>'])\n",
        "                inputs = line[:stop_no]\n",
        "                targets = line[1:stop_no+1]\n",
        "\n",
        "                # forward seq_len characters through the net and fetch gradient\n",
        "                h_prev = np.zeros((self.n_hid,1))\n",
        "                loss, x, h, y, prob = self.forward(inputs, targets, h_prev)\n",
        "                t_loss += loss\n",
        "                dWxh, dWhh, dWhy, dbh, dby, h = self.backward(inputs, targets, h_prev, x, h, y, prob)\n",
        "    \n",
        "                # perform parameter update with Adagrad\n",
        "                for param, delta, Hessian in zip([self.Wxh, self.Whh, self.Why, self.bh, self.by], \n",
        "                                                  [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                                  [self.mWxh, self.mWhh, self.mWhy, self.mbh, self.mby]):\n",
        "                    Hessian += delta * delta\n",
        "                    param += -self.lr * delta / np.sqrt(Hessian + 1e-8) # adagrad update\n",
        "            \n",
        "                self.smooth_loss = self.smooth_loss * 0.999 + t_loss * 0.001\n",
        "                self.losses.append(self.smooth_loss)\n",
        " \n",
        "            if epoch % self.interval == 0:\n",
        "                h_prev = np.zeros((self.n_hid,1))\n",
        "                sample_ix = self.sample(h_prev, inputs[0], 30)\n",
        "                txt = ''.join(self.idx2chr[ix] for ix in sample_ix)\n",
        "                #txt = re.sub('<eow>','\\n',txt)\n",
        "                print(f'--- 反復訓練数={epoch} 損失値: {self.smooth_loss:.3f} ---\\n{txt}')\n",
        "\n",
        "        print(f'--- 最終反復訓練数={epoch} 損失値: {self.smooth_loss:.3f} ---\\n{txt}')\n",
        "        if self.generate_text: self.all_text_generation()\n",
        "        if self.plot: self.draw_loss_plot(self.losses)\n",
        "        return {'Wxh':self.Wxh, 'Whh':self.Whh, 'Why':self.Why, 'bh':self.bh, 'by':self.by}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze_xqvzrrmb6"
      },
      "source": [
        "model = srn_model(max_epochs=10 ** 3, n_hid=20) #, generate_text=False, plot=False) \n",
        "_ = model.train(max_epochs=10 ** 3, plot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBkknfy9rvjv"
      },
      "source": [
        "gurenge = \"\"\"強くなれる理由を知った　僕を連れて進め\\n\n",
        "泥だらけの走馬灯に酔う　こわばる心\\n\n",
        "震える手は掴みたいものがある　それだけさ\\n\n",
        "夜の匂いに空睨んでも\\n\n",
        "変わっていけるのは自分自身だけ　それだけさ\\n\n",
        "強くなれる理由を知った　僕を連れて進め\\n\n",
        "どうしたって！\\n\n",
        "消せない夢も　止まれない今も\\n\n",
        "誰かのために強くなれるなら\\n\n",
        "ありがとう　悲しみよ\\n\n",
        "世界に打ちのめされて負ける意味を知った\\n\n",
        "紅蓮の華よ咲き誇れ！　運命を照らして\"\"\"\n",
        "\n",
        "# 外部でデータを定義したときのテスト\n",
        "X = gurenge.split('\\n')\n",
        "#print(X)\n",
        "\n",
        "model = srn_model(text=X, max_epochs=10 ** 2, n_hid=16, lr=1e-1)\n",
        "#_ = model.train(plot=False)\n",
        "_ = model.train(plot=True)\n",
        "\n",
        "# 外部ファイルを読み込んだときのテスト\n",
        "# X = open('hosomichi.txt').read().strip().split('\\n')\n",
        "# print(X)\n",
        "# model = onomatope_generator(text=X, max_epochs=10 ** 2, lr=1e-1)\n",
        "# model.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28d98HnWtoQU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}